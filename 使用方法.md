~/dev/nora-xiaozhi-dev/3rd/vadc/build $ arecord -f S16_LE -c 1 -r 16000 -q - | ./vadc --stats

════════════════════════════════════════════
📋 程序参数配置:
  输入源: stdin
  说话概率阈值: 0.50
  最小沉默时长: 200 ms
  最小说话时长: 250 ms
  原始概率输出: 否
  统计信息输出: ✓ 启用
  详细日志: 否
════════════════════════════════════════════

⏳ 等待 stdin 的音频数据 (16kHz, 16-bit, mono PCM)...
   按 Ctrl+C 停止
   提示：可以通过以下方式提供音频:
   - arecord -f S16_LE -c 1 -r 16000 -q - | ./vadc --stats
   - ffmpeg -i file.wav -f s16le -ac 1 -ar 16000 - | ./vadc --stats


🚀 初始化推理引擎...
  模型路径: 
Found model: ../silero_vad_v4.onnx
Loading ONNX model: ../silero_vad_v4.onnx
Running with batch size 1
Running with sequence count 1536
✓ 初始化完成
🎵 开始处理音频数据...
🎤 检测到语音事件 | 时间: 1.15-1.63秒 (时长: 0.48秒) | 概率: 5.0%
1.12,1.66
time=00:00:03.0839 | speech=0.54s (14.1%) | total=3.8s | speed=1.0x
🎤 检测到语音事件 | 时间: 3.17-3.55秒 (时长: 0.38秒) | 概率: 11.1%
3.14,3.58
time=00:00:07.0488 | speech=0.98s (13.1%) | total=7.5s | speed=1.0x
🎤 检测到语音事件 | 时间: 6.14-7.20秒 (时长: 1.06秒) | 概率: 8.8%
6.11,7.23
time=00:00:14.0784 | speech=2.10s (14.2%) | total=14.8s | speed=1.0x
🎤 检测到语音事件 | 时间: 11.33-14.50秒 (时长: 3.17秒) | 概率: 24.3%

你看到了把，现在vadc的测试方法如上，功能很好，速度很快，占用内存也很少




我的意思是你不能直接参考 vad.c 的main方法来直接通过调用process_chunks run_inference等vadc.h 中的方法来完成吗：
具体可以复制vadc.c 一份为 libvadc.c不包含main方法的实现

这样我们先不集成vadc到小智，我们先来把vadc改造成支持编译成lib库
保留原来vadc.c 作为测试例子使用，复制vadc.c 一份为 libvadc.c不包含main方法的实现，然后我们参考vadc.c 中的main方法提取出来初始化和销毁等一些必要的方法添加到vadc.h中去，然后根据这个库api再参考vadc.c 中的main的使用方法来写一个测试例子，这个例子能和原来main程序输出一样的效果的时候，我们再集成到xiaozhi中去，你懂我的意思吧！